{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249020bc-61ea-4c01-93e6-b47db4d4dcd0",
   "metadata": {},
   "source": [
    "# Chapter 2 - Fairness & Proxy Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f7a5fa-5370-4346-8528-eae6da82e812",
   "metadata": {},
   "source": [
    "# **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cba9f-0581-4f0a-ab9d-aa20171169db",
   "metadata": {},
   "source": [
    "Humans, even as young as 3 years old, exhibit a natural inclination towards fairness and merit when sharing resources.\n",
    "Studies on primates like chimpanzees suggest that the sense of fairness is not exclusive to humans; they also respond to inequity.\n",
    "Despite this innate understanding of fairness, real-world experiences often reveal biases and prejudices leading to discrimination.\n",
    "Discriminatory biases can be deliberate (e.g., based on skin color), stereotype-driven (e.g., gender roles in certain jobs), or subconscious, reflecting past societal practices.\n",
    "In business contexts, these biases manifest in discriminatory actions, favoring certain groups (the privileged class) over others (unprivileged classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f41a08-7e7b-49ce-91e6-52082bd26aa2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Example of Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4be4e137-f2b3-410a-b71d-b4504262d74c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe src=\"https://www.tiktok.com/embed/v2/7255381463892364550\" width=\"325\" height=\"580\" frameborder=\"0\" allow=\"autoplay; clipboard-write; encrypted-media; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<iframe src=\"https://www.tiktok.com/embed/v2/7255381463892364550\" width=\"325\" height=\"580\" frameborder=\"0\" allow=\"autoplay; clipboard-write; encrypted-media; picture-in-picture\" allowfullscreen></iframe>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2734618-5f72-4a56-94dd-8036463bc71c",
   "metadata": {},
   "source": [
    "## Key terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1d94d-99dd-4bfc-a3d5-bd0e8d47164e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Key concepts:\n",
    "\n",
    "* **Favorable outcome**: outcome that users desire (a mortgage approval, configuration of a Bluetooth device, a good picture). Estimated and Actual symbols correspondingly: $ \\hat{Y}_{\\text{fav}} $ , $ Y_{\\text{fav}} $\n",
    "\n",
    "* **Unfavorable outcome**: outcome the user does not desire.\n",
    "Estimated and Actual symbols correspondingly: $ \\hat{Y}_{\\text{unfav}} $ , $ Y_{\\text{unfav}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0dcca-a79b-4b61-8f84-85afc39b66bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Protected Features**\n",
    "\n",
    "Features in our datasets can typically be divided into 2 groups:\n",
    "\n",
    "**Independent features** are those that **DO NOT** contain any personal, racial or socio-economic indicators that may be used for discrimination; as opposed to **Protected Features** which contain such information.\n",
    "\n",
    "Examples of protected features can vary significantly based on the context where we operate (including the regulations of the country or industry we work in), but typically include:\n",
    "\n",
    "<details>\n",
    "  <summary style=\"background-color: #f0f0f0; padding: 10px; border: 1px solid #ccc; width: fit-content; cursor: pointer;\">\n",
    "    Click to see Protected Features\n",
    "  </summary>\n",
    "\n",
    "* Race (Civil Rights Act of 1964)\n",
    "* Color\n",
    "* Sex including gender, pregnancy, sexual orientation, and gender identity (Equal Pay Act of 1963)\n",
    "* Religion or creed\n",
    "* National origin or ancestry\n",
    "* Citizenship (Immigration Reform and Control Act)\n",
    "* Age (Age Discrimination in Employment Act of 1967)\n",
    "* Pregnancy (Pregnancy Discrimination Act)\n",
    "* Familial status\n",
    "* Physical or mental disability status (Rehabilitation Act of 1973)\n",
    "* Veteran status\n",
    "* Genetic information\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "***Protected Features*** are identified by the letter ***S***\n",
    "\n",
    "The set of the remaining **Independent Features** are represented by uppercase **X**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82355c81-3ea5-43ba-9393-3f3e15f00b04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Implementation of Fairness Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e884b-4504-4182-99bd-dbced7ab9c1d",
   "metadata": {},
   "source": [
    "When dealing with protected features, we can identify user groups or classes. For example, for marital status, we could identify *married*, *divorced*, or *single* as 3 possible groups/classes.\n",
    "\n",
    "Depending on the problem we are trying to solve, one group could be more likely to receive *favorable outcomes* (treatment) over the other.\n",
    "\n",
    "- The class with increased likelihood of receiving favorable outcomes is referred to as the **Privilege Class** and identified by S<sub>a</sub>, \n",
    "\n",
    "- While the **Unprivileged Class** is identified by S<sub>d</sub>.\n",
    "\n",
    "Bias reduction approaches aim to reduce this gap.\n",
    "\n",
    "Let's define concepts and metrics that will help us identify this gap. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d71fb-57e6-4e25-8cd9-b61eb292a506",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **2. Confussion Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada867f6-0b46-4bdf-9acc-c39c028a81b7",
   "metadata": {},
   "source": [
    "By definition a confusion matrix M is such that is equal to the number of observations known to be in group Y<sub>i</sub> and predicted to be in group Y<sub>j</sub>.\n",
    "\n",
    "Y<sub>i</sub> -> actual class or what the true label is.\n",
    "Y<sub>j</sub> -> predicted class or what the model predicts.\n",
    "\n",
    "The quadrants of the confusion matrix are:\n",
    "\n",
    "**1. True Positive (TP)**: predictive and actual outcomes are both in the positive class. ($ \\hat{Y}_{\\text{fav}} $ + $ Y_{\\text{fav}} $)\n",
    "\n",
    "**2. False Positive (FP)**: predicted values are in positive class, but actual outcome is in negative class. ($ Y_{\\text{fav}} $ + $ Y_{\\text{unfav}} $)\n",
    "\n",
    "**3. False Negative (FN)**: predicted values are in negative class, but actual outcome is in positive class. ( $ \\hat{Y}_{\\text{unfav}} $ + $ Y_{\\text{fav}} $)\n",
    "\n",
    "**4. True Negative (TN)**: predicted and actual outcomes are both in the negative class.( $ \\hat{Y}_{\\text{unfav}} $ + $ Y_{\\text{unfav}} $)\n",
    "\n",
    "We'll use these to create accuracy & Fairness metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a4ef0-b206-4c5d-bf7f-b25528f1801a",
   "metadata": {},
   "source": [
    "![Confusion Matrix](./img/confusion_matrix2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c1694-c455-44fd-8c19-76d967296922",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Confusion Matrix](./img/confusion_matrix1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8f094-f138-4002-9113-2df2cb33a440",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2a. Accuracy Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968e648-cd93-4733-80bf-8f2454df4285",
   "metadata": {},
   "source": [
    "* **False Positive Rate (FPR)** is the probability of raising a false alarm, calculated as **FP / (FP + TN)**. Businesses aim to minimize FPR due to its potentially costly nature, especially in financial services.\n",
    "\n",
    "* **False Negative Rate (FNR)**, or the **miss rate**, is the probability of missing a true positive, calculated as **FN / (FN + TP)**. This can also incur significant costs for a business.\n",
    "\n",
    "* **True Positive Rate (TPR)**, or **sensitivity/recall**, is the probability of correctly identifying positives, calculated as **TP / (TP + FN)**. Increasing TPR is crucial for achieving favourable outcomes.\n",
    "\n",
    "* **True Negative Rate (TNR)**, or **specificity**, is the probability of correctly identifying negatives, calculated as **TN / (TN + FP)**. TNR is important, particularly when negative outcomes are costly.\n",
    "\n",
    "* The ratio of **TPR + FNR = 1** (actual positive outcomes), and **FPR + TNR = 1** (actual negative outcomes).\n",
    "\n",
    "* **Positive Predictive Value (PPV)**, or **precision**, is the fraction of positive cases correctly predicted out of all predicted positive cases, calculated as **TP / (TP + FP)**. Precision measures the ability of the classifier to avoid labeling negative samples as positive.\n",
    "\n",
    "* **F1 Score**, is a metric that seeks to balance precision and recall. It identifies the harmonic mean of precision and recall. F1 = 2 x ((precisionxrecall)/(precision+recall))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3cbec-2736-495b-a788-e4322bf39d5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Common Accuracy Metrics](./img/common_accuracy_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504f383-db55-4f03-84fc-121f58e03900",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2b. Types of Fairness**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098957cd-b89f-475c-9014-e702aae221af",
   "metadata": {},
   "source": [
    "The concept of fairness revolves around majorly the below three concepts, that are based on the relationships between the protected feature and the actual and the predicted outcomes.\n",
    "\n",
    "* **Independence**: $ \\hat{Y} $ is independent of S if predicted outcome $ \\hat{Y} $ and protected features **S** are unrelated. Being in a favourable or non-favourable class isn't influenced by the protected group. \n",
    "\n",
    "Example: In hiring decisions, the likelihood of being hired ($ \\hat{Y} $) is not dependent on religion (**S**).\n",
    "\n",
    "* **Separation**: Prediction ($ \\hat{Y}) $ is conditionally independent of protected features (**S**) given the actual target value ($ {Y} $), if predicted features given the target value Y are unrelated to the protected feature. The probability of being in any class, given the actual class, is independent of protected group membership.\n",
    "\n",
    "Example: In the criminal justice system, the prediction of whether someone will re-offend should be equally accurate for different racial groups, conditional on the actual re-offense status.\n",
    "\n",
    "* **Sufficiency**: The actual outcome ($ {Y}$) is conditionally Independent of **S**, given the predicted outcome ($ \\hat{Y}$). That is, the actual outcome shouldn't depent on group membership once the model has made a prediction.\n",
    "\n",
    "Here, the protected class should be independent of the actual value given the predicted value. Prediction shouldn't rely on the protected group's membership.\n",
    "\n",
    "Example: In loan approvals, the likelihood of actually repaying a loan (${Y}$) shoud not be dependent on race (**S**), given that the model has predicted the likelihood of repayment ($\\hat{Y}$).\n",
    "\n",
    "***Summary***\n",
    "\n",
    "* Independence: No dependency between predictions and protected group.\n",
    "* Separation: Fairness in prediction, when considering the actual outcome.\n",
    "* Sufficiency: The prediction should fully encapsulate all relevant information about the actual outcome. Therefore, the protected group should be irrelevant.\n",
    "\n",
    "($ \\hat{Y}_{\\text{fav}} $ + $ Y_{\\text{fav}} $)\n",
    "( $ \\hat{Y}_{\\text{unfav}} $ + $ Y_{\\text{unfav}} $)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ead8ee-f9d0-4077-b469-f590a2be887b",
   "metadata": {},
   "source": [
    "### **2b. NOTE: Fairness Impossibility Theorem**\n",
    "also known as triadic fairness trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a356a-f503-4763-ae11-eeef8d5b76bb",
   "metadata": {},
   "source": [
    "It is generally considered impossible to maximize all 3 fairness criteria at once as their requirements conflict with each other.\n",
    "\n",
    "**1. Incompatibility of Independence and Separation**\n",
    "\n",
    "\n",
    "**2. Conflict between Separation and Sufficiency**\n",
    "Separation focuses on ensuring that prediction outcomes are accurate across different groups given the actual outcomes. However, Sufficiency aims that given the predicted outcome ($\\hat{Y}$), the actual outcome (${Y}$) are independent of the protected feature.\n",
    "\n",
    "**3. Idenpendence vs. Sufficiency**\n",
    "Independence requires that protected features have no influence over the prediction. However, sufficiency requires that given a prediction, the actual outcomes should not differ by protected groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759088a2-fe2d-4c0f-a0a0-ee0fd2a79fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4311ac29-9f50-415c-baf0-0d93bc38b711",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3. Fairness Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd38bf-9fad-45cf-8caa-286ea90fd411",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **1. Equal Opportunity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c20a1f-f0e3-406d-9362-13e6f8881a03",
   "metadata": {},
   "source": [
    "Establishes that both priviledge and unprivileged groups have **equal False Negative Rates (FNR)**.\n",
    "\n",
    "Note: Since TPR + FNR = 1, this also implies that there will be **equal TPR**.\n",
    "\n",
    "$$ P\\left( \\hat{Y} = 0 \\mid Y = 1, S = S_a \\right) = P\\left( \\hat{Y} = 0 \\mid Y = 1, S = S_d \\right)$$\n",
    "\n",
    "This means that the probability that the prediction is negative ($\\hat{Y}$) given that the actual value was positive (${Y}$) is the same for the priviledge class ($ S_a $) as it is for the unpriviledge class ($ S_d $)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26636a6-aefb-49cc-91fe-6153a3c3cec0",
   "metadata": {},
   "source": [
    "* Both privileged and unprivileged groups must have equal False Negative Rates (FNR) for a classifier to be considered fair. \n",
    "\n",
    "* Equal FNR implies that the likelihood of a non-defaulting loan applicant being wrongly classified as a defaulter is the same across all groups within a protected feature. \n",
    "\n",
    "* Ensuring no group faces a higher error rate is crucial; no group should suffer from an increased rate of incorrect predictions by the model. \n",
    "\n",
    "* Since the sum of True Positive Rate (TPR) and FNR equals 1, equal FNRs result in equal TPRs, enhancing fairness by maintaining consistent sensitivity/recall across groups. \n",
    "\n",
    "* An example provided shows differing FNRs for Males (0.273) and Females (0.667), indicating that Women do not have equal opportunity for favourable outcomes compared to Men, thereby identifying Women as the unprivileged class in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e341c4-bbe2-4c9b-9c05-c389887859a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **2. Predictive Equality**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc0454-5383-47b0-8dde-de058be80791",
   "metadata": {},
   "source": [
    "Both priviledge and unpriviledge groups have **equal FPR**.\n",
    "\n",
    "$$ P\\left( \\hat{Y} = 1 \\mid Y = 0, S = S_a \\right) = P\\left( \\hat{Y} = 1 \\mid Y = 0, S = S_d \\right) $$\n",
    "\n",
    "This means that the probability that the prediction is positive ($\\hat{Y}$) given that the actual value was negative (${Y}$) is the same for the priviledge class ($ S_a $) as it is for the unpriviledge class ($ S_d $)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd3c27-f34a-431c-a276-f6d43e750c65",
   "metadata": {},
   "source": [
    "* Both privileged and unprivileged groups must have equal False Positive Rates (FPR). \n",
    "\n",
    "* Equal FPR means the chance of an actual defaulter being wrongly predicted as a non-defaulter should be identical across all subsets within a protected class. \n",
    "\n",
    "* The example provided shows FPR for Males at 0.5 and for Females at 0.333, indicating Males are more likely to be incorrectly assessed as low risk compared to Females. \n",
    "\n",
    "* This discrepancy confirms the existence of bias, with Males more likely to benefit from an error in their favor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f924cd-8a1c-4426-93bc-cc545b84a108",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **3. Equalized Odds**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb43884-5ea0-410b-83b8-f328ce79929e",
   "metadata": {},
   "source": [
    "Also known as disparate mistreatment, equalized odds requires that both priviledged and unpriviledged groups have **equal TPR and FPR**.\n",
    "\n",
    "$$ P\\left( \\hat{Y} = 1 \\mid Y = i, S = S_a \\right) = P\\left( \\hat{Y} = 1 \\mid Y = i, S = S_d \\right), i \\in \\{0, 1\\} $$\n",
    "\n",
    "* $\\hat{Y}$ represents the predicted outcome.\n",
    "* ${Y}$ **= i** where i can be 0 or 1 represents the actual outcome (0 for negative, 1 for positive).\n",
    "* $S_a$ and $S_d$ represent two protected groups (e.g., privileged and unprivileged).\n",
    "* $i \\in \\{0, 1\\}$ means **i** can take values **0 or 1**, which includes both possible outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e0651-aff4-4206-b500-632fc649d919",
   "metadata": {},
   "source": [
    "* A classifier meets this criterion if it ensures both advantageous and disadvantageous groups have identical TPR and FPR. \n",
    "\n",
    "* This means the likelihood of correctly predicting a non-defaulter as such, and the chance of wrongly predicting a defaulter as a non-defaulter, must be equal across all protected group members. \n",
    "\n",
    "* The core principle is that applicants, regardless of their protected class status, should receive similar classification outcomes based on their actual creditworthiness. \n",
    "\n",
    "* The choice of metric (TPR or FPR) depends on the specific use case or underlying principles, highlighting how different metrics can lead to varied interpretations of fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67220b93-eb75-443d-a57f-71297a1f1483",
   "metadata": {},
   "source": [
    "### **4. Predictive Parity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93fb6ff-1cc4-424c-adf4-af8fbf9dc55b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Predictive Parity is also known as outcome test. It requires both advantageous and disadvantageous groups to have **equal Precision** (also called Positive Predictive Value or PPV).\n",
    "\n",
    "$$ P\\left( Y = 1 \\mid \\hat{Y} = 1, S = S_a \\right) = P\\left( Y = 1 \\mid \\hat{Y} = 1, S = S_d \\right) $$\n",
    "\n",
    "* $\\hat{Y}$ = 1  represents the predicted positive outcome.\n",
    "* ${Y}$ = 1  represents the actual positive outcome.\n",
    "* $S_a$  and  $S_d$  represent two protected groups (e.g., privileged and unprivileged).\n",
    "\n",
    "This formula expresses Predictive Parity, which requires that, given a positive prediction ($\\hat{Y}$ = 1), the probability of the actual outcome being positive (Y = 1) should be the same for both protected groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f18974-1e6a-43d6-ab58-aff0f1ebcda5",
   "metadata": {},
   "source": [
    "* Predictive parity, or the outcome test, requires equal Positive Predictive Value (PPV)/precision for both privileged and unprivileged groups. \n",
    "\n",
    "* The goal is to ensure that the rate of correct positive predictions is consistent across all groups within a protected class, distributing errors evenly. \n",
    "\n",
    "* This ensures that the likelihood of receiving a favourable outcome is independent of group membership. \n",
    "\n",
    "* In practice, this means that the likelihood of correctly identifying a non-defaulting loan applicant should be uniform across advantageous and disadvantageous groups. \n",
    "\n",
    "* A significant advantage of predictive parity is that a model with perfect prediction (Precision of 1 for all groups) is deemed fair. \n",
    "\n",
    "* However, achieving predictive parity doesn't necessarily imply bias reduction or elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f8c98-52a5-4d3c-8f3f-22606b716abe",
   "metadata": {},
   "source": [
    "### **5. Demographic Parity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5852795-332c-4416-aa1d-7b4dbf1f3bff",
   "metadata": {
    "tags": []
   },
   "source": [
    "Membership in a protected class should have no correlation with being predicted a favourable outcome.\n",
    "\n",
    "$$P\\left( \\hat{Y} = 1, S = S_a \\right) = P\\left( \\hat{Y} = 1, S = S_d \\right)$$\n",
    "\n",
    "* $\\hat{Y}$ = 1  represents the predicted positive outcome.\n",
    "* $S_a$  and  $S_d$  represent two protected groups (e.g., privileged and unprivileged).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27c895-affa-448d-a0eb-dc9db821ea3e",
   "metadata": {},
   "source": [
    "* Membership in a protected class should not affect the likelihood of being predicted to achieve a favourable outcome.\n",
    "\n",
    "* The demographic distribution of predicted favourable outcomes should align with the application pool's composition.\n",
    "\n",
    "* Achieving demographic parity requires adjustments in confusion matrices for both privileged and unprivileged groups:\n",
    "    - For the privileged group, the focus is on reducing false positives and increasing true negatives, minimizing business costs and preventing unfair advantages.\n",
    "    - For the unprivileged group, efforts aim to decrease false negatives and increase true positives, improving the likelihood of deserving individuals receiving favourable outcomes and fostering positive discrimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4a94c-22c2-4e74-af95-eb760e871546",
   "metadata": {},
   "source": [
    "### **6. Average Odds Difference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801cdb8-c41f-4ca8-838b-a0097ea37d65",
   "metadata": {},
   "source": [
    "Average of difference in FPR and TPR for advantageous and disadvantageous groups.\n",
    "\n",
    "$$ \\frac{1}{2} \\left[ \\left( FPR_{S_d} - FPR_{S_a} \\right) + \\left( TPR_{S_d} - TPR_{S_a} \\right) \\right]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eff2a1-e480-470a-aae0-472f3ed2e663",
   "metadata": {},
   "source": [
    "* This metric measures the average difference between False Positive Rates (FPR) and True Positive Rates (TPR) across privileged and underprivileged groups, incorporating both predictive equality difference (from FPR to TNR)** and **equal opportunity difference.\n",
    "\n",
    "* **A lower or zero difference indicates equal benefits (favorable outcomes) for both groups**.\n",
    "\n",
    "* Ideal scenarios would show nearly identical, if not identical, fairness metric values across protected classes, indicating minimal to no difference.\n",
    "\n",
    "* The example demonstrates significant disparities in fairness metrics between males and females, highlighting discrimination against the protected group \"Gender = F\" (Females as the unprivileged group).\n",
    "\n",
    "* To accurately assess privileged versus unprivileged groups, these metrics must be calculated for all protected features and their respective groups.\n",
    "\n",
    "* After identifying privileged and unprivileged groups, prioritize relevant metrics for the specific problem being addressed and track these metrics during bias mitigation efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86098c6e-de7b-418f-9ddf-57aa8efead35",
   "metadata": {},
   "source": [
    "![Fairness Metrics](./img/Summary_Fairness_Metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6df72-3851-4526-a032-5e4a657b8fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **7. Calculating Fairness Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361bfd4-7034-430d-a994-4fcc199d2afe",
   "metadata": {},
   "source": [
    "Let's create a function to calculate these metrics for us.\n",
    "\n",
    "We'll represent our metrics of interest with the following parameter names:\n",
    "\n",
    "* y_actual -> ${Y}$\n",
    "\n",
    "* y_pred_prob -> predicted probability produced by the model. Probability of an instance of belonging to a positive class.\n",
    "\n",
    "* y_pred_binary -> $\\hat{Y}$\n",
    "\n",
    "* X_test -> The set of independent features.\n",
    "\n",
    "* protected_group_name -> Sensitive Feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "96262a19-9b2e-4c69-bd61-f46e08b42a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pages 20 & 21\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score\n",
    "\n",
    "def fair_metrics(y_actual, y_pred_prob, y_pred_binary, X_test, protected_group_name, adv_val, disadv_val):\n",
    "    \"\"\"\n",
    "    Fairness performance metrics for a model to compare advantageous and disadvantageous groups of a protected variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    :param y_actual: Actual binary outcome\n",
    "    :param y_pred_prob: Predicted probabilities\n",
    "    :param y_pred_binary: Predicted binary outcome\n",
    "    :param X_test: X_test data\n",
    "    :param protected_group_name: Sensitive feature \n",
    "    :param adv_val: Privileged value of protected label\n",
    "    :param disadv_val: Unprivileged value of protected label\n",
    "    :return: roc, avg precision, Eq of Opportunity, Equalised Odds, Precision/Predictive Parity, \n",
    "             Demographic Parity, Avg Odds, Diff, Predictive Equality, Treatment Equality\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    \n",
    "    fairness_metrics=[fair_metrics(y_test, y_pred_prob, y_pred, X_test, choice, adv_val, disadv_val)]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tn_adv, fp_adv, fn_adv, tp_adv = confusion_matrix(\n",
    "        y_actual[X_test[protected_group_name] == adv_val],\n",
    "        y_pred_binary[X_test[protected_group_name] == adv_val]\n",
    "    ).ravel()\n",
    "    \n",
    "    tn_disadv, fp_disadv, fn_disadv, tp_disadv = confusion_matrix(\n",
    "        y_actual[X_test[protected_group_name] == disadv_val],\n",
    "        y_pred_binary[X_test[protected_group_name] == disadv_val]\n",
    "    ).ravel()\n",
    "    \n",
    "    # Receiver operating characteristic\n",
    "    roc_adv = roc_auc_score(\n",
    "        y_actual[X_test[protected_group_name] == adv_val],\n",
    "        y_pred_prob[X_test[protected_group_name] == adv_val]\n",
    "    )\n",
    "    \n",
    "    roc_disadv = roc_auc_score(\n",
    "        y_actual[X_test[protected_group_name] == disadv_val],\n",
    "        y_pred_prob[X_test[protected_group_name] == disadv_val]\n",
    "    )\n",
    "    \n",
    "    roc_diff = abs(roc_disadv - roc_adv)\n",
    "    \n",
    "    # Average precision score\n",
    "    ps_adv = average_precision_score(\n",
    "        y_actual[X_test[protected_group_name] == adv_val],\n",
    "        y_pred_prob[X_test[protected_group_name] == adv_val]\n",
    "    )\n",
    "                                     \n",
    "    ps_disadv = average_precision_score(\n",
    "        y_actual[X_test[protected_group_name] == disadv_val],\n",
    "        y_pred_prob[X_test[protected_group_name] == disadv_val]\n",
    "    )\n",
    "\n",
    "    ps_diff = abs(ps_disadv - ps_adv)\n",
    "    \n",
    "    # Equal Opportunity - advantageous and disadvantageous groups have equal FNR\n",
    "    FNR_adv = fn_adv / (fn_adv + tp_adv)\n",
    "    FNR_disadv = fn_disadv / (fn_disadv + tp_disadv)\n",
    "    EOpp_diff = abs(FNR_disadv - FNR_adv)\n",
    "    \n",
    "    # Predictive equality - advantageous and disadvantageous groups have equal FPR\n",
    "    FPR_adv = fp_adv / (fp_adv + tn_adv)\n",
    "    FPR_disadv = fp_disadv / (fp_disadv + tn_disadv)\n",
    "    pred_eq_diff = abs(FPR_disadv - FPR_adv)\n",
    "    \n",
    "    # Equalised Odds - advantageous and disadvantageous groups have equal TPR + FPR\n",
    "    TPR_adv = tp_adv / (tp_adv + fn_adv)\n",
    "    TPR_disadv = tp_disadv / (tp_disadv + fn_disadv)\n",
    "    EOdds_diff = abs((TPR_disadv + FPR_disadv) - (TPR_adv + FPR_adv))\n",
    "    \n",
    "    # Predictive Parity - advantageous and disadvantageous groups have equal PPV/Precision (TP/TP+FP)\n",
    "    prec_adv = tp_adv / (tp_adv + fp_adv)\n",
    "    prec_disadv = tp_disadv / (tp_disadv + fp_disadv)\n",
    "    prec_diff = abs(prec_disadv - prec_adv)\n",
    "    \n",
    "    # Demographic Parity - ratio of (instances with favorable prediction) / (total instances)\n",
    "    demo_parity_adv = (tp_adv + fp_adv) / (tn_adv + fp_adv + fn_adv + tp_adv)\n",
    "    demo_parity_disadv = (tp_disadv + fp_disadv) / (tn_disadv + fp_disadv + fn_disadv + tp_disadv)\n",
    "    demo_parity_diff = abs(demo_parity_disadv - demo_parity_adv)\n",
    "    \n",
    "    # Average of Difference in FPR and TPR for advantageous and disadvantageous groups\n",
    "    AOD = 0.5 * ((FPR_disadv - FPR_adv) + (TPR_disadv - TPR_adv))\n",
    "    \n",
    "    # Treatment Equality - advantageous and disadvantageous groups have equal ratio of FN/FP\n",
    "    TE_adv = fn_adv / fp_adv\n",
    "    TE_disadv = fn_disadv / fp_disadv\n",
    "    TE_diff = abs(TE_disadv - TE_adv)\n",
    "    \n",
    "    return [\n",
    "        ('AUC', roc_diff), ('Avg PrecScore', ps_diff), ('Equal Opps', EOpp_diff),\n",
    "        ('PredEq', pred_eq_diff), ('Equal Odds', EOdds_diff), ('PredParity', prec_diff),\n",
    "        ('DemoParity', demo_parity_diff), ('AOD', abs(AOD)), ('TEq', TE_diff)\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ec556-172e-4fe9-a3ae-1fe1630c1b8c",
   "metadata": {},
   "source": [
    "#### Testing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "80d048c6-36d6-45fe-b4e8-22e8d484b041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.0128\n",
      "Equal Opps: 0.0092\n",
      "PredEq: 0.0072\n",
      "Equal Odds: 0.0163\n",
      "PredParity: 0.2370\n",
      "DemoParity: 0.0538\n",
      "AOD: 0.0082\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \n",
    "           \"hours-per-week\", \"native-country\", \"income\"]\n",
    "data = pd.read_csv(url, header=None, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Convert categorical variables to numeric\n",
    "for col in ['workclass', 'education', 'marital-status', 'occupation', 'relationship', \n",
    "            'race', 'sex', 'native-country', 'income']:\n",
    "    data[col] = LabelEncoder().fit_transform(data[col])\n",
    "\n",
    "# Define the features and the target variable\n",
    "X = data.drop('income', axis=1)  # Features\n",
    "y = data['income']  # Target: 0 for <=50K, 1 for >50K\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]  # Predicted probabilities\n",
    "y_pred = model.predict(X_test)  # Predicted binary outcomes\n",
    "\n",
    "# Step 4: Define the fairness metrics function\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score\n",
    "\n",
    "def fair_metrics(y_actual, y_pred_prob, y_pred_binary, X_test, protected_group_name, adv_val, disadv_val):\n",
    "    \"\"\"\n",
    "    Fairness performance metrics for a model to compare advantageous and disadvantageous groups of a protected variable.\n",
    "    \"\"\"\n",
    "    tn_adv, fp_adv, fn_adv, tp_adv = confusion_matrix(\n",
    "        y_actual[X_test[protected_group_name] == adv_val],\n",
    "        y_pred_binary[X_test[protected_group_name] == adv_val]\n",
    "    ).ravel()\n",
    "\n",
    "    tn_disadv, fp_disadv, fn_disadv, tp_disadv = confusion_matrix(\n",
    "        y_actual[X_test[protected_group_name] == disadv_val],\n",
    "        y_pred_binary[X_test[protected_group_name] == disadv_val]\n",
    "    ).ravel()\n",
    "\n",
    "    # ROC AUC\n",
    "    roc_adv = roc_auc_score(\n",
    "        y_actual[X_test[protected_group_name] == adv_val],\n",
    "        y_pred_prob[X_test[protected_group_name] == adv_val]\n",
    "    )\n",
    "\n",
    "    roc_disadv = roc_auc_score(\n",
    "        y_actual[X_test[protected_group_name] == disadv_val],\n",
    "        y_pred_prob[X_test[protected_group_name] == disadv_val]\n",
    "    )\n",
    "    roc_diff = abs(roc_disadv - roc_adv)\n",
    "\n",
    "    # Equal Opportunity Difference (FNR difference)\n",
    "    FNR_adv = fn_adv / (fn_adv + tp_adv)\n",
    "    FNR_disadv = fn_disadv / (fn_disadv + tp_disadv)\n",
    "    EOpp_diff = abs(FNR_disadv - FNR_adv)\n",
    "\n",
    "    # Predictive Equality (FPR difference)\n",
    "    FPR_adv = fp_adv / (fp_adv + tn_adv)\n",
    "    FPR_disadv = fp_disadv / (fp_disadv + tn_disadv)\n",
    "    pred_eq_diff = abs(FPR_disadv - FPR_adv)\n",
    "\n",
    "    # Equalized Odds Difference (TPR + FPR difference)\n",
    "    TPR_adv = tp_adv / (tp_adv + fn_adv)\n",
    "    TPR_disadv = tp_disadv / (tp_disadv + fn_disadv)\n",
    "    EOdds_diff = abs((TPR_disadv + FPR_disadv) - (TPR_adv + FPR_adv))\n",
    "\n",
    "    # Predictive Parity Difference (Precision difference)\n",
    "    prec_adv = tp_adv / (tp_adv + fp_adv)\n",
    "    prec_disadv = tp_disadv / (tp_disadv + fp_disadv)\n",
    "    prec_diff = abs(prec_disadv - prec_adv)\n",
    "\n",
    "    # Demographic Parity Difference (positive prediction rate difference)\n",
    "    demo_parity_adv = (tp_adv + fp_adv) / (tn_adv + fp_adv + fn_adv + tp_adv)\n",
    "    demo_parity_disadv = (tp_disadv + fp_disadv) / (tn_disadv + fp_disadv + fn_disadv + tp_disadv)\n",
    "    demo_parity_diff = abs(demo_parity_disadv - demo_parity_adv)\n",
    "\n",
    "    # Average Odds Difference\n",
    "    AOD = 0.5 * ((FPR_disadv - FPR_adv) + (TPR_disadv - TPR_adv))\n",
    "\n",
    "    return [\n",
    "        ('AUC', roc_diff), ('Equal Opps', EOpp_diff), ('PredEq', pred_eq_diff),\n",
    "        ('Equal Odds', EOdds_diff), ('PredParity', prec_diff),\n",
    "        ('DemoParity', demo_parity_diff), ('AOD', abs(AOD))\n",
    "    ]\n",
    "\n",
    "# Step 5: Calculate fairness metrics for 'sex' (gender)\n",
    "protected_group_name = 'sex'  # 'sex' column in the dataset (0: Female, 1: Male)\n",
    "adv_val = 1  # Advantageous group: Male\n",
    "disadv_val = 0  # Disadvantageous group: Female\n",
    "\n",
    "fairness_metrics = fair_metrics(y_test, y_pred_prob, y_pred, X_test, protected_group_name, adv_val, disadv_val)\n",
    "\n",
    "# Display the fairness metrics\n",
    "for metric, value in fairness_metrics:\n",
    "    print(f'{metric}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8dd729-c830-43ac-97cd-8d44e5e837db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **CREATE FAIRNESS METRICS PYTHON FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e948d-3a5a-4a37-8ea0-ae0933ff0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fairness_metrics.py\n",
    "\n",
    "# You'll be able to import this by using \n",
    "\n",
    "# from fariness_metrics import fair_metrics\n",
    "\n",
    "# fair_metrics(y_actual, y_pred_prob, y_pred_binary, X_test, protected_group_name, adv_val, disadv_val)\n",
    "\n",
    "# Pages 20 & 21\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score\n",
    "\n",
    "def fair_metrics(y_actual, y_pred_prob, y_pred_binary, X_test, protected_group_name, adv_val, disadv_val):\n",
    "    \"\"\"\n",
    "    Fairness performance metrics for a model to compare advantageous and disadvantageous groups of a protected variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    :param y_actual: Actual binary outcome\n",
    "    :param y_pred_prob: Predicted probabilities\n",
    "    :param y_pred_binary: Predicted binary outcome\n",
    "    :param X_test: X_test data\n",
    "    :param protected_group_name: Sensitive feature \n",
    "    :param adv_val: Privileged value of protected label\n",
    "    :param disadv_val: Unprivileged value of protected label\n",
    "    :return: roc, avg precision, Eq of Opportunity, Equalised Odds, Precision/Predictive Parity, \n",
    "             Demographic Parity, Avg Odds, Diff, Predictive Equality, Treatment Equality\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    \n",
    "    fairness_metrics=[fair_metrics(y_test, y_pred_prob, y_pred, X_test, choice, adv_val, disadv_val)]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tn_adv, fp_adv, fn_adv, tp_adv = confusion_matrix(\n",
    "        y_actual[X_test[protected_group_name] == adv_val],\n",
    "        y_pred_binary[X_test[protected_group_name] == adv_val]\n",
    "    ).ravel()\n",
    "    \n",
    "    tn_disadv, fp_disadv, fn_disadv, tp_disadv = confusion_matrix(\n",
    "        y_actual[X_test[protected_group_name] == disadv_val],\n",
    "        y_pred_binary[X_test[protected_group_name] == disadv_val]\n",
    "    ).ravel()\n",
    "    \n",
    "    # Receiver operating characteristic\n",
    "    roc_adv = roc_auc_score(\n",
    "        y_actual[X_test[protected_group_name] == adv_val],\n",
    "        y_pred_prob[X_test[protected_group_name] == adv_val]\n",
    "    )\n",
    "    \n",
    "    roc_disadv = roc_auc_score(\n",
    "        y_actual[X_test[protected_group_name] == disadv_val],\n",
    "        y_pred_prob[X_test[protected_group_name] == disadv_val]\n",
    "    )\n",
    "    \n",
    "    roc_diff = abs(roc_disadv - roc_adv)\n",
    "    \n",
    "    # Average precision score\n",
    "    ps_adv = average_precision_score(\n",
    "        y_actual[X_test[protected_group_name] == adv_val],\n",
    "        y_pred_prob[X_test[protected_group_name] == adv_val]\n",
    "    )\n",
    "                                     \n",
    "    ps_disadv = average_precision_score(\n",
    "        y_actual[X_test[protected_group_name] == disadv_val],\n",
    "        y_pred_prob[X_test[protected_group_name] == disadv_val]\n",
    "    )\n",
    "\n",
    "    ps_diff = abs(ps_disadv - ps_adv)\n",
    "    \n",
    "    # Equal Opportunity - advantageous and disadvantageous groups have equal FNR\n",
    "    FNR_adv = fn_adv / (fn_adv + tp_adv)\n",
    "    FNR_disadv = fn_disadv / (fn_disadv + tp_disadv)\n",
    "    EOpp_diff = abs(FNR_disadv - FNR_adv)\n",
    "    \n",
    "    # Predictive equality - advantageous and disadvantageous groups have equal FPR\n",
    "    FPR_adv = fp_adv / (fp_adv + tn_adv)\n",
    "    FPR_disadv = fp_disadv / (fp_disadv + tn_disadv)\n",
    "    pred_eq_diff = abs(FPR_disadv - FPR_adv)\n",
    "    \n",
    "    # Equalised Odds - advantageous and disadvantageous groups have equal TPR + FPR\n",
    "    TPR_adv = tp_adv / (tp_adv + fn_adv)\n",
    "    TPR_disadv = tp_disadv / (tp_disadv + fn_disadv)\n",
    "    EOdds_diff = abs((TPR_disadv + FPR_disadv) - (TPR_adv + FPR_adv))\n",
    "    \n",
    "    # Predictive Parity - advantageous and disadvantageous groups have equal PPV/Precision (TP/TP+FP)\n",
    "    prec_adv = tp_adv / (tp_adv + fp_adv)\n",
    "    prec_disadv = tp_disadv / (tp_disadv + fp_disadv)\n",
    "    prec_diff = abs(prec_disadv - prec_adv)\n",
    "    \n",
    "    # Demographic Parity - ratio of (instances with favorable prediction) / (total instances)\n",
    "    demo_parity_adv = (tp_adv + fp_adv) / (tn_adv + fp_adv + fn_adv + tp_adv)\n",
    "    demo_parity_disadv = (tp_disadv + fp_disadv) / (tn_disadv + fp_disadv + fn_disadv + tp_disadv)\n",
    "    demo_parity_diff = abs(demo_parity_disadv - demo_parity_adv)\n",
    "    \n",
    "    # Average of Difference in FPR and TPR for advantageous and disadvantageous groups\n",
    "    AOD = 0.5 * ((FPR_disadv - FPR_adv) + (TPR_disadv - TPR_adv))\n",
    "    \n",
    "    # Treatment Equality - advantageous and disadvantageous groups have equal ratio of FN/FP\n",
    "    TE_adv = fn_adv / fp_adv\n",
    "    TE_disadv = fn_disadv / fp_disadv\n",
    "    TE_diff = abs(TE_disadv - TE_adv)\n",
    "    \n",
    "    return [\n",
    "        ('AUC', roc_diff), ('Avg PrecScore', ps_diff), ('Equal Opps', EOpp_diff),\n",
    "        ('PredEq', pred_eq_diff), ('Equal Odds', EOdds_diff), ('PredParity', prec_diff),\n",
    "        ('DemoParity', demo_parity_diff), ('AOD', abs(AOD)), ('TEq', TE_diff)\n",
    "    ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
